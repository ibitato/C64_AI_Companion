# C64 AI Companion

C64 AI Companion is a reproducible fine-tuning project that adapts a reasoning-capable Ministral 3 8B model to technical Commodore 64 knowledge.

## Project Objective

The objective is to keep strong reasoning behavior while adding accurate, practical C64 technical knowledge for topics such as BASIC, KERNAL, memory map, VIC-II, SID, and 6502/6510 workflows.

## What This Project Is and Is Not

- It is a container-first training and packaging workflow for one controlled base model.
- It is a reproducible engineering pipeline for dataset preparation, DAPT/SFT fine-tuning, and GGUF export.
- It is not a generic multi-model framework.
- It does not use user-global model caches as authoritative model paths.

## Architecture and Reproducibility

- Training and packaging run in Docker.
- Canonical training image: `rocm/pytorch:rocm7.2_ubuntu22.04_py3.10_pytorch_release_2.9.1`.
- Canonical base model path: `models/Ministral-3-8B-Thinking`.
- Project-local cache: `.cache/huggingface`.

### Strix Halo Runtime Note

This project intentionally documents a split between host runtime details and container runtime details.

- Host runtime is infrastructure context.
- Container runtime is the training source of truth for reproducibility.
- For this workstation profile, container ROCm/HIP 7.x userland is the supported training runtime for Strix Halo compatibility.

See `docs/workstation_profile.md` for the host/container compatibility matrix.

## Quickstart

1. Export your host user IDs for container file ownership:

```bash
export LOCAL_UID=$(id -u)
export LOCAL_GID=$(id -g)
```

2. Build image:

```bash
docker compose build trainer
```

3. Run GPU smoke test:

```bash
docker compose run --rm trainer bash scripts/container/gpu_smoke.sh
```

4. Build datasets:

```bash
docker compose run --rm trainer bash scripts/container/pipeline.sh
```

5. Train (DAPT + SFT):

```bash
docker compose run --rm trainer bash scripts/container/train.sh
```

6. Export GGUF:

```bash
docker compose run --rm trainer bash scripts/container/export_gguf.sh \
  --base-model-path models/Ministral-3-8B-Thinking \
  --adapter-path models/fine-tuned \
  --gguf-dir models/gguf \
  --quantization Q4_K_M
```

7. Optional extra quantizations (`Q6_K`, `Q8_0`):

```bash
bash scripts/inference/quantize_additional_gguf.sh
```

8. Reproducible GGUF benchmark matrix (container-run, CSV output):

```bash
bash scripts/inference/benchmark_gguf_matrix.sh
```

## Inference Runtimes

- Ollama helper:

```bash
bash scripts/inference/create_ollama_models.sh
```

- llama.cpp helper:

```bash
bash scripts/inference/run_llama_cpp.sh Q6_K "Explain SID voices in two concise points."
```

- Benchmark all GGUF variants and write `results/benchmarks/*.csv`:

```bash
bash scripts/inference/benchmark_gguf_matrix.sh --models "F16 Q4_K_M Q6_K Q8_0"
```

## Hugging Face Artifacts

- Collection: https://huggingface.co/collections/ibitato/c64-ministral-3-8b-thinking-c64-reasoning-699d67350911049ec1a82e18
- LoRA repo: https://huggingface.co/ibitato/c64-ministral-3-8b-thinking-c64-reasoning-lora
- GGUF repo: https://huggingface.co/ibitato/c64-ministral-3-8b-thinking-c64-reasoning-gguf

## Publication Traceability

This repository and the Hugging Face artifacts are maintained as a bidirectional publication set:

- GitHub -> Hugging Face: this `README` links to the LoRA and GGUF model repositories.
- Hugging Face -> GitHub: both model cards link back to this repository (`https://github.com/ibitato/C64_AI_Companion`).
- Reproducibility metadata (trainer states, training summary, model cards) is generated by `scripts/release/publish_hf.py` from local artifacts.

For operational publishing details, see `docs/release_huggingface.md`.

## Repository Structure

```text
C64_AI_Companion/
|-- c64_docs/                    # Source C64 manuals used for dataset generation
|-- data/                        # Interim and processed datasets
|-- docs/                        # Project documentation and operational manuals
|-- models/                      # Base model, fine-tuned outputs, GGUF exports
|-- scripts/                     # Data pipeline, training, export, runtime automation
|-- tests/                       # GPU and data-pipeline validation tests
|-- Dockerfile.train
|-- docker-compose.yml
`-- requirements*.txt
```

## Documentation Index

Start at `docs/index.md`.

## Security and Model Policy

- Base model path is restricted by policy to `models/Ministral-3-8B-Thinking`.
- Sensitive tokens must be stored in local `.env` only.
- Large artifacts and caches remain excluded by `.gitignore`.

## AI Co-Authors

This project includes AI-assisted engineering contributions from:

- Mistral AI Devstral 2
- Mistral AI Vibe CLI
- Codex 5.3
- Codex CLI

See `CREDITS.md` for contribution scope and responsibility boundaries.
